{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using explainers\n",
    "\n",
    "The previous section was focused on training models using local resources or with cloud resources that either you bring or you use from the nitrain.dev platform. Training models is the culmination of all the previous sections: handling images, creating datasets, generating batches, and creating models.\n",
    "\n",
    "However, the process is not over when a model is trained to perfection. For medical imaging tasks, it is equally important to know \"how\" and \"why\" the model gives the predictions it does. That is what explaining results is all about.\n",
    "\n",
    "In this section, you will learn to use explainers in nitrain. Explainers are basically algorithms that run data through your model to better understand the parts of images to which your model pays closest attention. By using explainers, you can better understand how a trained model reaches the conclusions it does.\n",
    "\n",
    "The process of explaining results in nitrain can be compared to the beta coefficients from a regression model. Those coefficients are important in dispelling the incorrect notion that medical imaging AI models are black boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
